{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5a5652da",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from dotenv import load_dotenv\n",
                "from IPython.display import display, HTML, Markdown\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "5ed08b77",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents import create_agent\n",
                "from langchain_aws import ChatBedrock\n",
                "\n",
                "# 1. CONFIGURACIÓN PARA DEEPSEEK-R1 (Razonamiento Complejo)\n",
                "# Ideal para agentes que necesitan planificar pasos lógicos.\n",
                "# llm = ChatBedrock(\n",
                "#     model_id=\"us.deepseek.r1-v1:0\",  # ID oficial validado\n",
                "#     region_name=\"us-east-1\",\n",
                "#     model_kwargs={\n",
                "#         \"temperature\": 0.6, # DeepSeek recomienda 0.6 para razonamiento\n",
                "#         \"max_tokens\": 8192,  # Recomendado para no degradar calidad del CoT\n",
                "#         \"top_p\": 0.95,\n",
                "#     }\n",
                "# )\n",
                "\n",
                "\n",
                "# llm = ChatBedrock(\n",
                "#     model_id=\"us.deepseek.v3-v1:0\", # Prueba este primero\n",
                "#     region_name=\"us-east-1\",        # O us-west-2\n",
                "#     model_kwargs={\n",
                "#         \"temperature\": 0.7,\n",
                "#         \"max_tokens\": 4096\n",
                "#     }\n",
                "# )\n",
                "from langchain_aws import ChatBedrock\n",
                "llm_scout = ChatBedrock(\n",
                "model_id=\"us.meta.llama4-scout-17b-instruct-v1:0\",  # Nota el prefijo \"us.\"\n",
                "# model_id=\"cohere.command-r-plus-v1:0\",\n",
                "region_name=\"us-east-1\",\n",
                "model_kwargs={\n",
                "\"temperature\": 0.5,\n",
                "\"max_tokens\": 2048,\n",
                "\"top_p\": 0.9,\n",
                "}\n",
                ")\n",
                "\n",
                "\n",
                "# llm = ChatBedrock(\n",
                "#     model_id=\"us.meta.llama4-maverick-17b-instruct-v1:0\",  # Nota el prefijo \"us.\"\n",
                "#     region_name=\"us-east-1\",\n",
                "#     model_kwargs={\n",
                "#         \"temperature\": 0.5,\n",
                "#         \"max_tokens\": 2048,\n",
                "#         \"top_p\": 0.9,\n",
                "#     }\n",
                "# )\n",
                "\n",
                "llm_novalite = ChatBedrock(\n",
                "    model_id=\"amazon.nova-lite-v1:0\",  # Nota el prefijo \"us.\"\n",
                "    region_name=\"us-east-1\",\n",
                "    model_kwargs={\n",
                "        \"temperature\": 0.5,\n",
                "        \"max_tokens\": 2048,\n",
                "        \"top_p\": 0.9,\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "e4c157d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
                "from langchain.chat_models import init_chat_model\n",
                "from typing import Callable\n",
                "\n",
                "large_model = init_chat_model(\"us.meta.llama4-scout-17b-instruct-v1:0\",\n",
                "    model_provider=\"bedrock\",\n",
                "    region_name=\"us-east-1\")\n",
                "standard_model = init_chat_model(\"amazon.nova-lite-v1:0\", \n",
                "    model_provider=\"bedrock\",\n",
                "    region_name=\"us-east-1\")\n",
                "\n",
                "\n",
                "@wrap_model_call\n",
                "def state_based_model(request: ModelRequest, \n",
                "handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
                "    \"\"\"Select model based on State conversation length.\"\"\"\n",
                "    # request.messages is a shortcut for request.state[\"messages\"]\n",
                "    message_count = len(request.messages)  \n",
                "\n",
                "    if message_count > 10:\n",
                "        # Long conversation - use model with larger context window\n",
                "        model = large_model\n",
                "    else:\n",
                "        # Short conversation - use efficient model\n",
                "        model = standard_model\n",
                "\n",
                "    request = request.override(model=model)  \n",
                "\n",
                "    return handler(request)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "608cb293",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_aws import ChatBedrock\n",
                "\n",
                "llm = ChatBedrock(\n",
                "    model_id=\"us.meta.llama4-maverick-17b-instruct-v1:0\",  # Nota el prefijo \"us.\"\n",
                "    region_name=\"us-east-1\",\n",
                "    model_kwargs={\n",
                "        \"temperature\": 0.5,\n",
                "        \"max_tokens\": 2048,\n",
                "        \"top_p\": 0.9,\n",
                "    }\n",
                ")\n",
                "\n",
                "from langchain.agents import create_agent\n",
                "from langchain.agents import create_agent\n",
                "\n",
                "agent = create_agent(\n",
                "    model=llm,\n",
                "    middleware=[state_based_model],\n",
                "    system_prompt=\"You are roleplaying a real life helpful office intern.\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65753916",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Hi there! I'm not able to physically water the office plant, but I can certainly remind everyone to do it. It's important to keep our office environment lively and healthy. If you have a watering schedule or if someone is responsible for it, I can send a reminder via email or our team chat. Let me know how you'd like to proceed!\n"
                    ]
                }
            ],
            "source": [
                "from langchain.messages import HumanMessage\n",
                "\n",
                "response = agent.invoke(\n",
                "    {\"messages\": [\n",
                "        HumanMessage(content=\"Did you water the office plant today?\")\n",
                "        ]}\n",
                ")\n",
                " \n",
                "print(response[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "a1f4e44c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "amazon.nova-lite-v1:0\n"
                    ]
                }
            ],
            "source": [
                "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "76c8ff48",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "I'd say in about 3-4 months, once it's outgrown its current pot. I'll keep an eye on the roots and let you know when it's time to transplant it into a larger one.\n"
                    ]
                }
            ],
            "source": [
                "from langchain.messages import AIMessage\n",
                "\n",
                "response = agent.invoke(\n",
                "    {\"messages\": [\n",
                "        HumanMessage(content=\"Did you water the office plant today?\"),\n",
                "        AIMessage(content=\"Yes, I gave it a light watering this morning.\"),\n",
                "        HumanMessage(content=\"Has it grown much this week?\"),\n",
                "        AIMessage(content=\"It's sprouted two new leaves since Monday.\"),\n",
                "        HumanMessage(content=\"Are the leaves still turning yellow on the edges?\"),\n",
                "        AIMessage(content=\"A little, but it's looking healthier overall.\"),\n",
                "        HumanMessage(content=\"Did you remember to rotate the pot toward the window?\"),\n",
                "        AIMessage(content=\"I rotated it a quarter turn so it gets more even light.\"),\n",
                "        HumanMessage(content=\"How often should we be fertilizing this plant?\"),\n",
                "        AIMessage(content=\"About once every two weeks with a diluted liquid fertilizer.\"),\n",
                "        HumanMessage(content=\"When should we expect to have to replace the pot?\")\n",
                "        ]}\n",
                ")\n",
                "\n",
                "print(response[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "a54183a1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "us.meta.llama4-scout-17b-instruct-v1:0\n"
                    ]
                }
            ],
            "source": [
                "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eaa88d27",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
